[
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: “DevOps on AWS” Event Event Objectives Understand DevOps culture and mindset: Introduce the core principles of DevOps, emphasizing collaboration, automation, and continuous improvement.\nLearn about AWS DevOps tools: Dive into AWS services that support DevOps practices, including CI/CD pipelines, Infrastructure as Code (IaC), containerization, and monitoring.\nExplore DevOps best practices: Understand key metrics like DORA and MTTR, and learn best practices for deployment, incident management, and observability.\nDemonstrate real-world use cases: Showcase AWS DevOps tools through demos and case studies, illustrating how these tools can be applied to various scenarios.\nKey Highlights Welcome \u0026amp; DevOps Mindset Recap of AI/ML session: A brief review of the previous session on AI/ML.\nDevOps culture and principles: Introduction to the cultural shift towards collaboration, automation, and continuous delivery.\nBenefits and key metrics: Overview of key DevOps metrics such as DORA (DevOps Research and Assessment), MTTR (Mean Time to Recovery), and deployment frequency.\nAWS DevOps Services – CI/CD Pipeline Source Control: Introduction to AWS CodeCommit and Git strategies like GitFlow and Trunk-based development.\nBuild \u0026amp; Test: Setting up AWS CodeBuild for automated builds and testing in CI/CD pipelines.\nDeployment: Learn about AWS CodeDeploy, with strategies for Blue/Green, Canary, and Rolling updates.\nOrchestration: AWS CodePipeline for automating the CI/CD pipeline from source to deployment.\nDemo: Full walkthrough of a complete CI/CD pipeline setup using AWS services.\nInfrastructure as Code (IaC) AWS CloudFormation: Overview of CloudFormation templates, stacks, and drift detection to manage infrastructure as code.\nAWS CDK (Cloud Development Kit): Introduction to the AWS CDK, with reusable patterns and language support.\nDemo: Deploying infrastructure using CloudFormation and CDK.\nDiscussion: Choosing between CloudFormation and CDK for IaC implementation.\nContainer Services on AWS Docker Fundamentals: An introduction to Docker, microservices, and containerization.\nAmazon ECR: Learn about Elastic Container Registry (ECR) for image storage, scanning, and lifecycle policies.\nAmazon ECS \u0026amp; EKS: Explore deployment strategies, scaling, and orchestration with Elastic Container Service (ECS) and Elastic Kubernetes Service (EKS).\nAWS App Runner: Simplified container deployment using AWS App Runner.\nDemo \u0026amp; Case Study: Comparison of microservices deployment using ECS, EKS, and App Runner.\nMonitoring \u0026amp; Observability CloudWatch: Setup and usage of AWS CloudWatch for metrics, logs, alarms, and dashboards.\nAWS X-Ray: Introduction to AWS X-Ray for distributed tracing and performance insights across microservices.\nDemo: Full-stack observability setup with CloudWatch and X-Ray for monitoring application health.\nBest Practices: Tips on alerting, dashboard setup, and creating effective on-call processes.\nDevOps Best Practices \u0026amp; Case Studies Deployment Strategies: Discuss advanced deployment strategies such as Feature flags and A/B testing for better control over application release.\nAutomated Testing and CI/CD Integration: How to integrate automated testing into your CI/CD pipelines for efficient testing and deployment.\nIncident Management and Postmortems: Best practices for handling incidents and conducting postmortem analyses to prevent future issues.\nCase Studies: Real-world examples of DevOps transformations in both startups and enterprises.\nQ\u0026amp;A \u0026amp; Wrap-up DevOps Career Pathways: Discuss various career opportunities in the DevOps field and how to develop the necessary skills.\nAWS Certification Roadmap: Overview of AWS certifications related to DevOps and how to prepare for them.\nWhat I Learned DevOps Culture and Principles: DevOps Mindset: The importance of collaboration between development and operations teams, fostering a culture of continuous improvement.\nKey Metrics: Understanding of DORA, MTTR, and deployment frequency as measures of DevOps success and efficiency.\nCI/CD Pipeline with AWS: AWS CodeCommit: Best practices for managing source code using AWS’s version control system.\nCodeBuild \u0026amp; CodeDeploy: How to configure automated build and deployment pipelines with AWS tools.\nCodePipeline Automation: The role of AWS CodePipeline in automating the entire software delivery lifecycle.\nInfrastructure as Code (IaC): AWS CloudFormation vs CDK: The benefits of using CloudFormation for managing resources declaratively, compared to the flexibility of AWS CDK for more programmatic infrastructure management. Containerization on AWS: ECR, ECS, and EKS: How to manage container images and deploy containerized applications at scale using AWS services.\nAWS App Runner: The ease of deploying applications using AWS App Runner for simpler containerization needs.\nMonitoring and Observability: CloudWatch \u0026amp; X-Ray: How to implement full-stack observability in a microservices architecture using AWS CloudWatch and X-Ray for better performance tracking and issue resolution. Application to My Work CI/CD Pipeline Setup: Implement a full CI/CD pipeline using AWS CodePipeline, CodeBuild, and CodeDeploy to automate the deployment process in my current projects.\nIaC with AWS CDK: Use AWS CDK for creating reusable infrastructure patterns and simplifying the management of cloud resources in my organization.\nContainerization with ECS \u0026amp; EKS: Deploy microservices using ECS and EKS, focusing on scaling and orchestration for better efficiency.\nImplement Observability: Set up CloudWatch and X-Ray for monitoring the health of applications in production and use insights to improve performance.\nExperience at the Event Learning from Experts: The speakers shared in-depth knowledge about DevOps principles, AWS DevOps tools, and real-world case studies, which helped me understand the implementation of DevOps practices in various environments.\nHands-on Demos: Participating in live demos and walking through the setup of CI/CD pipelines, IaC deployment, and container orchestration gave me practical insights into implementing these tools in my work.\nNetworking and Collaboration: The event allowed me to network with fellow professionals, exchange experiences, and gain insights into how other teams are implementing DevOps at scale.\nConclusion The “DevOps on AWS” event provided valuable insights into the world of DevOps and how AWS services can facilitate the implementation of modern software development practices. From automating the CI/CD pipeline to managing infrastructure as code, the event covered essential tools and strategies that can be applied directly to my work. I am excited to integrate these practices into my daily workflow to enhance automation, efficiency, and collaboration within my team.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 2 Objectives: Studied AWS Networking Services. Learn the theory and practice of basic EC2. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study the basic EC2 theory: + Instance types + AMI + EBS - remote SSH into EC2. - Study Elastic IP. - Practice: + Create EC2 instance + Connect SSH emsp; 09/14/2025 09/14/2025 https://000004.awsstudygroup.com/vi 3 - Studied AWS Networking Services + Amazon Virtual Private Cloud ( VPC ); + VPC Peering \u0026amp; Transit Gateway + VPN \u0026amp; Direct Connect + Elastic Load Balancing \u0026amp;emsp 09/15/2025 09/17/2025 https://www.youtube.com/watch?v=O9Ac_vGHquM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=25 4 - Studied AWS Networking Services + Amazon Virtual Private Cloud ( VPC ); + VPC Peering \u0026amp; Transit Gateway + VPN \u0026amp; Direct Connect + Elastic Load Balancing \u0026amp;emsp 09/15/2025 09/17/2025 https://000003.awsstudygroup.com/vi 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 09/16/2025 09/17/2025 https://000003.awsstudygroup.com/vi/1-introduce 6 - Group meeting about project ideas and writing worklog 09/18/2025 09/18/2025 Week 2 Achievements: Study the basic EC2 theory::\nInstance types: different virtual hardware configurations provided by AWS to run EC2 instances, each optimized for specific use cases. AMI: a template to launch EC2 instances, including the operating system, software, and volume configuration. Ways to remote SSH into EC2: using an SSH Client with a key pair, EC2 Instance Connect via browser, Session Manager without opening port 22, and PuTTY on Windows with a .ppk file. Elastic IP: a static, public IPv4 address allocated to an AWS account and attachable/detachable to EC2 instances, ensuring a fixed public IP for stable external access. Successfully created and connected to EC2 via SSH.\nStudy AWS Networking Services:\nAmazon Virtual Private Cloud (VPC): a customizable virtual network in AWS Cloud that allows you to create an isolated and secure networking environment. VPC Peering \u0026amp; Transit Gateway: VPC Peering directly connects two VPCs via private IPs (simple but no transitive routing), while Transit Gateway acts as a central hub to connect multiple VPCs and on-premises networks, supporting transitive routing, scalability, and centralized management. VPN \u0026amp; Direct Connect: VPN connects on-premises networks to AWS over the Internet via encrypted tunnels (easy to deploy but Internet-dependent), while Direct Connect provides a dedicated connection with higher stability, bandwidth, and lower latency, though costlier and slower to set up. Elastic Load Balancing (ELB): automatically distributes traffic across multiple resources to improve availability, scalability, and fault tolerance, with ALB (HTTP/HTTPS), NLB (high-performance TCP/UDP), and GWLB (virtual appliances). "
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 3 Objectives: Learn about Compute VM services on AWS. Practice with Compute VM services on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Compute VM services on AWS. + Amazon Elastic Compute Cloud (EC2) + Amazon Lightsail + Amazon EFS / FSX + AWS Application Migration Service (MGN)regulations 09/15/2025 09/16/2025 https://www.youtube.com/watch?v=-t5h4N6vfBs\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=72 3 - Learn about Compute VM services on AWS. + Amazon Elastic Compute Cloud (EC2) + Amazon Lightsail + Amazon EFS / FSX + AWS Application Migration Service (MGN)regulations 09/15/2025 09/16/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 4 - Practice: + Deploy AWS Backup to the System + Using AWS File Storage Gateway + Create SH3 Bucket + Using Autoscaling Group 09/17/2025 09/19/2025 https://000013.awsstudygroup.com/vi 5 - Practice: + Deploy AWS Backup to the System + Using AWS File Storage Gateway + Create SH3 Bucket + Using Autoscaling Group 09/17/2025 09/19/2025 https://000024.awsstudygroup.com 6 - Practice: + Deploy AWS Backup to the System + Using AWS File Storage Gateway + Create SH3 Bucket + Using Autoscaling Group 09/17/2025 09/19/2025 https://000024.awsstudygroup.com Week 3 Achievements: Understood AWS Compute VM services:\nAmazon Elastic Compute Cloud (EC2): Amazon EC2 is similar to a virtual or traditional physical server. It provides fast provisioning, strong resource scalability, and flexibility. Amazon Lightsail: a low-cost compute service (pricing starts at $3.5/month). Each Lightsail instance includes a data transfer allocation (cheaper than EC2), suitable for light workloads, dev/test environments, and applications that do not require high CPU usage continuously for more than 2 hours per day. Amazon EFS / FSx: EFS (Elastic File System): allows creation of NFSv4 network volumes that can be mounted to multiple EC2 instances simultaneously, with storage scaling up to petabytes. EFS only supports Linux and charges based on used storage. It can be mounted to on-premises environments via Direct Connect or VPN. FSx: allows creation of NTFS volumes mountable to multiple EC2 instances using the SMB protocol, supports Windows and Linux, and charges based on used storage. AWS Application Migration Service: used to migrate and replicate servers for building Disaster Recovery sites, continuously copying source physical or virtual servers to EC2 instances in AWS (asynchronously or synchronously). Successfully deployed AWS Backup, File Storage Gateway, and Auto Scaling Group..\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 4 Objectives: Discuss and brainstorm project ideas with the group. Learn about storage services on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about storage services on AWS. + Amazon Simple Storage Service - S3 + Amazon Storage Gateway + Snow Family + Disaster Recovery on AWS emsp; + AWS Backup 09/22/2025 09/23/2025 https://www.youtube.com/watch?v=hsCfP0IxoaM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=103 3 - Learn about storage services on AWS. + Amazon Simple Storage Service - S3 + Amazon Storage Gateway + Snow Family + Disaster Recovery on AWS emsp; + AWS Backup 09/22/2025 09/23/2025 https://www.youtube.com/watch?v=hsCfP0IxoaM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=103 4 - Perform Lab on AWS storage services. - Practice: + VM Import/Export + Deploy File Storage Gateway 09/24/2025 09/25/2025 https://000014.awsstudygroup.com/vi https://000024.awsstudygroup.com/vi 5 - Perform Lab on AWS storage services. - Practice: + VM Import/Export + Deploy File Storage Gateway 09/24/2025 09/25/2025 https://000014.awsstudygroup.com/vi https://000024.awsstudygroup.com/vi 6 - Group meeting about project ideas and writing worklog 09/26/2025 09/26/2025 Week 4 Achievements: Learned about AWS storage services:\nAmazon Simple Storage Service - S3 Amazon Storage Gateway Snow Familys Disaster Recovery on AWS Successfully imported/exported VM and deployed File Storage Gateway.\nCompleted writing the worklog and finalized the project idea.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 5 Objectives: Learn about security services on AWS. Do the lab. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about security services on AWS. + Shared Responsibility Model - AWS Identity and Access Management + Amazon Cognito + AWS Organization \u0026amp; AWS Identity Center ( SSO ) + AWS KMS 09/29/2025 09/30/2025 https://www.youtube.com/watch?v=tsobAlSg19g\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=150 3 - Learn about security services on AWS. + Shared Responsibility Model - AWS Identity and Access Management + Amazon Cognito + AWS Organization \u0026amp; AWS Identity Center ( SSO ) + AWS KMS 09/29/2025 09/30/2025 https://www.youtube.com/watch?v=tsobAlSg19g\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=150 4 Practice: + Compete Lab 18 about AWS Security Hub 10/01/2025 10/01/2025 https://000018.awsstudygroup.com 5 Practice: + Compete lab 22 \u0026amp; 27 about Optimizing EC2 Costs with Lambda và Manage Resources Using Tags and Resource Groups 10/02/2025 10/03/2025 https://000027.awsstudygroup.com https://000022.awsstudygroup.com 6 Practice: + Compete lab 22 \u0026amp; 27 about Optimizing EC2 Costs with Lambda và Manage Resources Using Tags and Resource Groups 10/02/2025 10/03/2025 https://000027.awsstudygroup.com https://000022.awsstudygroup.com Week 5 Achievements: Understand AWS security services:\nShared Responsibility Model: The AWS security model defines the division of responsibilities between AWS and customers in protecting systems and data on the cloud platform. AWS Identity and Access Management (IAM): Manages user identities, roles, and permissions to securely control access to AWS resources. Amazon Cognito: Provides authentication, authorization, and user management for web and mobile applications. AWS Organization \u0026amp; AWS Identity Center (SSO): Enables centralized management of multiple AWS accounts, unified access control, and single sign-on for users across the organization. AWS KMS: Manages encryption keys used to protect data, allowing secure creation, storage, and control of keys. Understand the structure of AWS Security Hub.\nSuccessfully completed Lab 22 \u0026amp; 27.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 6 Objectives: Continue working on the labs from Module 5. Understand basic AWS services and how to use both the AWS Management Console and AWS CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Practice: + Do lab 28 about MANAGE ACCESS TO EC2 SERVICES WITH RESOURCE TAGS THROUGH IAM SERVICES 10/06/2025 10/06/2025 https://000028.awsstudygroup.com 3 Practice: + Do lab 30 \u0026amp; 33 about LIMITATION OF USER RIGHTS WITH IAM PERMISSION BOUNDARY and Encrypt at rest with AWS KMS 10/07/2025 10/07/2025 https://000030.awsstudygroup.com https://000033.awsstudygroup.com 4 Practice: + Do lab 44 \u0026amp; 48 about IAM Role \u0026amp; Condition and Granting authorization for an application to access AWS services with an IAM role. 10/08/2025 10/08/2025 https://000044.awsstudygroup.com https://000048.awsstudygroup.com 5 - Learn about Database Services on AWS: + Database Concepts + Amazon RDS + Amazon Aurora + Amazon RedShift + Amazon ElastiCache 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=OOD2RwWuLRw\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=217 6 - Group meeting to draw the system diagram 10/10/2025 10/10/2025 Week 6 Achievements: Learned how to manage access to EC2 services using Resource Tags with AWS IAM. Successfully completed Lab 30 \u0026amp; 33, understanding IAM Permission Boundaries (limiting user permissions) and data encryption at rest using AWS KMS. Gained deeper understanding of IAM Roles \u0026amp; Conditions, and how to grant applications access to AWS services using IAM Roles. Acquired additional knowledge about AWS Database Services: Database Concepts Amazon RDS: A managed relational database service that supports multiple database engines such as MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server. It automates tasks like backup, patching, and scaling. Amazon Aurora: A high-performance, fully managed relational database compatible with MySQL and PostgreSQL, designed for scalability and high availability. Amazon RedShift: A fully managed data warehouse service optimized for large-scale data analysis (OLAP) and big data analytics. Amazon ElastiCache: A managed in-memory caching service supporting Redis and Memcached, improving application performance by reducing database load and query latency. "
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 7 Objectives: Complete the labs from module 6. Understand basic AWS services and how to use both the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Lab practice for module 6: + Complete Lab 5 on Amazon Relational Database Service (Amazon RDS) + Complete Lab 43 on schema conversion and database migration 10/13/2025 10/14/2025 https://000005.awsstudygroup.com https://000043.awsstudygroup.com 3 Lab practice for module 6: + Complete Lab 5 on Amazon Relational Database Service (Amazon RDS) + Complete Lab 43 on schema conversion and database migration 10/13/2025 10/14/2025 https://000005.awsstudygroup.com https://000043.awsstudygroup.com 4 - Research and work on the group project 10/15/2025 10/17/2025 5 - Research and work on the group project 10/15/2025 10/17/2025 6 - Research and work on the group project 10/15/2025 10/17/2025 Week 7 Achievements: Successfully completed Lab for module 6\nMaking good progress on researching and working on the group project\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 8 Objectives: Review lessons to prepare for the midterm exam Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 3 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 4 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 5 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 6 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 Week 8 Achievements: Still in the review process. "
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 9 Objectives: Review lessons and take the midterm exam. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review knowledge to prepare for the midterm exam 10/27/2025 10/29/2025 3 - Review knowledge to prepare for the midterm exam 10/27/2025 10/29/2025 4 - Review knowledge to prepare for the midterm exam 10/27/2025 10/29/2025 5 - Take the midterm exam 10/30/2025 10/30/2025 6 - Research and work on the group project 10/31/2025 10/31/2025 Week 9 Achievements: Finished reviewing and completed the midterm exam. "
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: “AI/ML/GenAI on AWS” Event Event Objectives Explore AWS AI/ML Services: Provide an overview of the AI/ML services available on AWS, helping participants understand how to apply these services in real-world projects.\nIntroduction to Generative AI: Focus on leveraging foundational models and Generative AI within AWS to build intelligent applications.\nLive Demo of Generative AI: Guide on building a chatbot using Amazon Bedrock, including techniques like prompt engineering and chain-of-thought reasoning.\nProvide Knowledge on MLOps and SageMaker: Introduce tools that manage the entire machine learning model lifecycle, from data preparation, model training to deployment and monitoring.\nKey Highlights Introduction to AI/ML on AWS: Amazon SageMaker: A comprehensive machine learning platform that helps deploy, train, and manage machine learning models.\nData Preparation and Labeling: Tools to assist with data preparation and labeling for machine learning models.\nMLOps Capabilities: Integrated features in SageMaker to support model operations (MLOps).\nLive Demo – SageMaker Studio Walkthrough: A demonstration of using SageMaker Studio to develop and deploy AI/ML models. Generative AI with Amazon Bedrock: Foundation Models: An introduction to foundational models such as Claude, Llama, Titan, and a guide to choosing the appropriate model.\nPrompt Engineering: Techniques for building effective prompts to optimize the performance of Generative AI models, including Chain-of-Thought reasoning and Few-shot learning.\nRetrieval-Augmented Generation (RAG): Architecture and integration of a knowledge base into the model generation process.\nBedrock Agents: Building multi-step workflows and integrating tools within Amazon Bedrock.\nGuardrails: Safety and content filtering measures when using Generative AI.\nLive Demo – Generative AI Chatbot with Bedrock: A demonstration of building a chatbot using Amazon Bedrock. What I Learned Design Thinking with AI/ML: Amazon SageMaker is the key platform supporting the full model lifecycle, from data preparation to model deployment and monitoring.\nPrompt Engineering: How to optimize Generative AI models using effective prompt-building techniques.\nMLOps: How to manage and automate model training, deployment, and monitoring processes in production environments.\nGenerative AI: Understanding foundational models and how they can create intelligent content, such as chatbots.\nAI/ML Architecture: Foundation Models: The differences between models like Claude, Llama, and Titan, and how to choose the right model based on project requirements.\nRAG (Retrieval-Augmented Generation): Understanding how RAG architecture works and how to integrate knowledge bases into the content creation process.\nBedrock Agents: Guidance on building complex workflows with pre-integrated tools.\nApplication to My Work Apply Amazon SageMaker: Integrate SageMaker tools into the data preparation, model training, and deployment processes for current projects.\nBuild a Generative AI Chatbot: Use Amazon Bedrock to develop chatbots and Generative AI applications for customer service or virtual assistant projects.\nOptimize AI/ML Models with Prompt Engineering: Apply prompt engineering techniques to improve AI models for better results.\nMLOps: Implement MLOps within the company to automate model training and monitoring, improving efficiency and reducing risks.\nExperience at the Event Learning from AWS Experts: The speakers shared in-depth knowledge about AI/ML and Generative AI, which helped me better understand how to use AWS tools for developing models.\nLive Demonstrations: I participated in live demos of SageMaker and Amazon Bedrock, which helped me visualize how to build and deploy AI models in real-world environments.\nNetworking and Interaction: The event allowed me to connect with other participants and experts, expanding my professional network and exchanging valuable AI/ML insights.\nConclusion The “AI/ML/GenAI on AWS” event provided me with valuable insights into the latest technologies in AI/ML, particularly around Generative AI and tools like SageMaker and Amazon Bedrock. Techniques like Prompt Engineering and MLOps have shown me the necessary steps to bring AI models from concept to production. I now feel more confident in applying these tools and techniques to real-world projects and advancing our AI/ML capabilities.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": " Deploying a High Performance Computing Solution for Accurate Weather and Renewable Energy Production Predictions The field of meteorology has long relied on computational models to predict weather patterns. The advent of High-Performance Computing (HPC) and machine learning (ML) has made these predictions more accurate and reliable. This post outlines the steps for deploying an HPC cluster for weather forecasting on the Amazon Web Services (AWS) Cloud. It examines how cloud-based solutions provide computational flexibility and scalability, demonstrating how HPC and cloud technologies help Iberdrola’s meteorologists calculate renewable energy production forecasts.\nFor Iberdrola—responsible for 80.3 TWh of renewable energy production in 2024—accurate weather forecasting is essential. Regulatory requirements implemented in Spain in 2004 mandated renewable energy production forecasting, leading Iberdrola Renovables to create MeteoFlow, an in-house forecasting tool with 20 years of continuous development.\nOver time, MeteoFlow has evolved to incorporate advanced meteorological simulation techniques, ML, AI, and big data technologies. With the support of multidisciplinary experts, the system now powers accurate short- and long-term energy production predictions across multiple renewable facilities.\nMeteoFlow generates production forecasts for Iberdrola’s global onshore wind farms, offshore wind farms, photovoltaic plants, and hydroelectric facilities—using wind predictions, solar radiation forecasts, and additional meteorological inputs. These forecasts support energy market participation, O\u0026amp;M planning, and risk assessment.\nModern meteorological models solve complex nonlinear partial differential equations such as the Navier–Stokes equations, requiring large-scale computation and high-throughput data access. These constraints make the cloud—with its elastic compute resources and virtually unlimited storage—a natural fit for running MeteoFlow forecasting workloads.\nBusiness Value MeteoFlow provides significant business value by producing accurate power generation forecasts up to:\n96 hours ahead (hourly) 10 days ahead (daily)\nfor 450+ wind and photovoltaic plants. Accurate forecasts help Iberdrola:\nImprove revenue Reduce penalties from market forecast deviations Increase renewable energy competitiveness Optimize O\u0026amp;M operations Improve risk management MeteoFlow also offers:\nWeather maps through Iberdrola’s intranet Real-time risk alerts (extreme winds, frost, storms) Flow prediction for hydro plants Wave prediction for offshore energy assets Data visualization and analysis via MeteoWeb MeteoFlow Architecture on AWS The system migration to AWS focused on three requirements:\nHandling 300+ TB of meteorological data at scale Real-time forecasting, enabling timely decision-making Flexibility and modularity for better integration and cross-team collaboration The architecture uses:\nAmazon S3 Stores global and regional meteorological model data (IFS, GFS, ICON-EU, custom internal models).\nAmazon EC2 HPC Instances Such as hpc7a.96xlarge:\n192 physical cores 768 GiB RAM EFA networking up to 300 Gbps 4th-gen AMD EPYC CPUs Optimized for tightly coupled HPC workloads Amazon EFS Elastic shared storage for HPC computation results.\nAmazon SageMaker Provides integrated analytics and ML model development for improved forecast precision.\nMigrating 300+ TB of Data to AWS Iberdrola initially considered AWS Snowball but discovered that their on-premises LAN bottleneck would make it no faster than transferring directly to AWS Ireland over their existing high-speed link.\nInstead, they used AWS DataSync, which:\nAutomates large data transfers Provides encryption and integrity validation Minimizes operational overhead High-level migration steps included:\nSetting up AWS Direct Connect Creating VGW \u0026amp; DX Gateway Configuring Private VIF Routing traffic via Direct Connect Deploying a DataSync agent on-prem Creating source \u0026amp; destination (S3) locations Running DataSync tasks This successfully migrated all historical MeteoFlow datasets to Amazon S3.\nConclusion Migrating MeteoFlow to the AWS Cloud marks a major milestone in its long development history. AWS enables Iberdrola to use elastic compute and storage, manage massive datasets, and leverage advanced services like Amazon SageMaker. With this cloud-native foundation, Iberdrola can focus on improving ML-driven forecasting models—ultimately strengthening renewable energy production and market performance.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": " Showcasing Your AWS Achievements with the New Skills Profile For learners looking to present their AWS journey with confidence and professionalism, AWS Skill Builder now offers Skills Profile—a powerful new way to highlight verified AWS certifications, learning achievements, and badges in one shareable profile. With customizable sharing options, Skills Profile helps you tell your unique cloud-skills story while building visibility and credibility in your industry.\nSkills Profile is available to Skill Builder users who are signed in, providing a centralized, trustworthy place to showcase your AWS accomplishments. Whether you are an aspiring cloud practitioner or an experienced AWS professional, your Skills Profile becomes your dedicated AWS skills showcase—ready to share on LinkedIn, in job applications, or with colleagues and hiring managers.\n“Skills Profile makes it easier to highlight the AWS learning journey and the skills I have developed. I can share my certifications and learning milestones in one clean, professional format—no screenshots or scattered links needed.”\n– AWS Skill Builder learner\nFrom Learning to Professional Recognition As social learning behaviors grow on professional platforms, it has become essential for learners to demonstrate their expertise in a trustworthy and consistent way. Skills Profile bridges the gap between private learning records and public professional recognition.\nWhile your Learner Dashboard remains private, the Skills Profile is designed for public sharing. It showcases:\nAWS Certifications Skill Builder learning achievement records Cloud Quest badges Other verified milestones You have full control over what appears on your profile, allowing you to tailor the story you share with your professional network.\nBuilding Connections Through Shared Skills Skills Profile delivers new value for both individuals and organizations by making cloud expertise visible, shareable, and verifiable.\nWhen you share your AWS achievements, you open opportunities for:\nProfessional networking Project collaboration Career advancement For organizations and hiring managers, Skills Profile becomes a trusted source for:\nDiscovering cloud talent Verifying AWS skill sets Matching roles with specific AWS capabilities In the future, Skills Profile will expand further, including:\nAdditional shareable achievements Features that help organizations and recruiters find talent based on AWS competencies Getting Started with Skills Profile To begin using Skills Profile:\nSign in to AWS Skill Builder Go to your profile settings Create and customize your Skills Profile Select which achievements to display Add a personalized headline Share your profile link with your network Whether you\u0026rsquo;re aiming for your next job role, showcasing new certifications, or sharing your learning progress with pride, Skills Profile helps you present your AWS expertise to the world.\nReady to share your AWS story?\nSign in to AWS Skill Builder today and create your Skills Profile!\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": " How DTN accelerates operational weather prediction using NVIDIA Earth-2 on AWS Accurate and timely weather forecasting is becoming increasingly essential for industries such as energy, agriculture, aviation, and maritime operations. Traditional physics-based numerical weather prediction (NWP) models require significant compute resources and time, making it challenging to scale during high-impact weather events. To address this, DTN collaborated with NVIDIA and AWS to integrate NVIDIA Earth-2, an AI-driven weather modeling platform, into their production system—enabling faster, more scalable, and highly accurate weather predictions.\nThis blog explains how DTN built a cloud-native, GPU-accelerated forecasting workflow on AWS using Earth2Studio, AWS Batch, EC2 GPU instances, and Step Functions. It also outlines how the system improves cyclone tracking accuracy, reduces operational risk, and supports mission-critical decision-making for DTN’s global customers.\nOverview of NVIDIA Earth-2 NVIDIA Earth-2 is an enterprise-grade platform designed for developing and running AI-powered weather and climate models. It provides:\nAccelerated pipelines for training physics-informed AI models Pre-trained weather models such as FourCastNet High-resolution super-resolution models like CorrDiff Visualization and simulation workflows Integration tools for real-time data from sources like the Registry of Open Data on AWS AI-based inference dramatically reduces forecast runtime—from hours to seconds—allowing organizations to generate large ensemble predictions quickly and cost-effectively.\nDTN’s AI Weather Forecasting Pipeline on AWS DTN implemented a fully managed, scalable inference pipeline using Earth2Studio and AWS cloud services. A typical forecast workflow includes:\nData Preparation\nAWS Lambda prepares input weather conditions Python utilities format the data for ingestion AI Model Inference\nAWS Batch deploys Earth2Studio workloads on EC2 GPU instances (G6e, P5, P6) FourCastNet generates ensemble forecasts at 25-km global resolution within seconds Post-Processing \u0026amp; Aggregation\nA Python container computes ensemble statistics Forecast uncertainty is quantified using measures such as standard deviation This architecture enables DTN to scale forecast generation on demand—especially useful for cyclone seasons or extreme weather events.\nCloud-Native Architecture DTN’s production system is built using containerized workflows and event-driven orchestration.\nKey AWS components: Amazon ECR – stores GPU-optimized inference containers AWS Batch – schedules and manages GPU/CPU workloads AWS Step Functions – orchestrates multi-stage forecasting pipelines Amazon S3 – central data lake for inputs, outputs, and model artifacts Amazon SNS – triggers notifications for downstream processes s3fs-mounted S3 volumes – accelerate access to large meteorological datasets To ensure reliability, the solution integrates:\nStep Functions retry logic Dead-letter queues for fault isolation Cached input data to reduce repeated downloads Full infrastructure-as-code deployment via AWS CDK Validation and Performance DTN validated the solution using real historical events such as Hurricanes Milton, Helene, and Lee. Ensemble forecasts produced by the Earth-2 models showed strong agreement with observed “BestTrack” data, illustrating the model’s ability to generate accurate cyclone tracks rapidly and consistently.\nThe system went live in June 2025 after a four-month development cycle, leveraging AWS and NVIDIA expertise to harden the workflow for production reliability.\nBusiness Impact By integrating AI-driven forecasts into its operational systems, DTN provides:\nMore accurate and timely weather predictions Better risk assessment for high-impact weather Improved operational margins for customers Enhanced decision-support dashboards using DTN’s “Decision-Grade Data” This allows clients across energy, agriculture, logistics, and supply chain industries to prepare, respond, and plan with greater confidence.\nFuture Roadmap DTN plans to continue expanding its AI forecasting capabilities, including:\nLonger forecasting horizons (from hourly to sub-seasonal) More advanced ensemble confidence metrics Additional AI models supported by the Earth-2 platform Integration with broader climate intelligence systems The flexibility of NVIDIA Earth-2 and AWS cloud services positions DTN to remain at the forefront of AI-driven weather innovation.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Summary Report: “AWS Well-Architected Security Pillar” Event Event Objectives Understand the Security Pillar of the AWS Well-Architected Framework: Gain insights into security best practices and how they apply to cloud architectures, focusing on AWS services and tools.\nLearn core security principles: Explore key principles such as Least Privilege, Zero Trust, and Defense in Depth to implement robust security in cloud environments.\nRecognize cloud security threats in Vietnam: Discuss common security challenges and threats faced by organizations in Vietnam’s cloud environment.\nDive deep into key AWS security pillars: Understand the five key security pillars—Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response—and how to implement them effectively.\nKey Highlights Opening \u0026amp; Security Foundation Security Pillar in Well-Architected Framework: Introduction to the importance of the Security Pillar in the AWS Well-Architected Framework and how it ensures a secure architecture.\nCore principles: Focus on Least Privilege, Zero Trust, and Defense in Depth as foundational concepts for designing secure systems.\nShared Responsibility Model: Understanding the division of responsibility between AWS and the customer for security in the cloud.\nTop threats in Vietnam\u0026rsquo;s cloud environment: Identifying the most common security threats faced by organizations operating in Vietnam\u0026rsquo;s cloud environment.\nPillar 1 — Identity \u0026amp; Access Management Modern IAM Architecture: Overview of Identity and Access Management (IAM) in AWS, including users, roles, and policies, and the importance of avoiding long-term credentials.\nIAM Identity Center: Introduction to Single Sign-On (SSO) and permission sets for managing access across AWS accounts.\nSCP \u0026amp; Permission Boundaries: How to use Service Control Policies (SCP) and permission boundaries to manage multi-account environments securely.\nMFA, Credential Rotation, Access Analyzer: Techniques for ensuring secure access through Multi-Factor Authentication (MFA), credential rotation, and access analysis.\nMini Demo: Validate IAM Policy and simulate access to see how IAM works in practice.\nPillar 2 — Detection Detection \u0026amp; Continuous Monitoring: Introduction to key services like CloudTrail, GuardDuty, and Security Hub for monitoring and detecting security events in AWS environments.\nLogging at every layer: Discuss the importance of logging at different layers (e.g., VPC Flow Logs, ALB/S3 logs) for comprehensive security monitoring.\nAlerting \u0026amp; Automation with EventBridge: Learn how to set up EventBridge for automated alerting and incident responses.\nDetection-as-Code: Implementing infrastructure and security rules using Detection-as-Code to automate the detection process.\nPillar 3 — Infrastructure Protection Network \u0026amp; Workload Security: Understanding the importance of VPC segmentation and private vs public placement in securing network traffic.\nSecurity Groups vs NACLs: Discuss the difference between Security Groups and Network Access Control Lists (NACLs) and which model is appropriate for different scenarios.\nWAF + Shield + Network Firewall: Protecting applications and networks using AWS WAF (Web Application Firewall), Shield, and Network Firewall to mitigate threats.\nWorkload Protection: Best practices for securing EC2 instances, ECS, and EKS clusters to ensure workload security.\nPillar 4 — Data Protection Encryption, Keys \u0026amp; Secrets: Introduction to AWS KMS (Key Management Service), including key policies, grants, and key rotation for secure data management.\nEncryption at Rest \u0026amp; in Transit: How to ensure encryption of data at rest (e.g., S3, EBS, RDS) and in transit (e.g., DynamoDB).\nSecrets Management: Using Secrets Manager and Parameter Store for managing sensitive data like passwords, keys, and tokens with automated rotation patterns.\nData Classification \u0026amp; Access Guardrails: Implementing data classification and access controls to ensure data protection.\nPillar 5 — Incident Response IR Playbook \u0026amp; Automation: The lifecycle of an Incident Response (IR) according to AWS, and how to automate the response process using AWS tools.\nIR Playbook: Common scenarios like a compromised IAM key, S3 public exposure, and EC2 malware detection, and how to handle them effectively.\nAuto-response with Lambda/Step Functions: Using AWS Lambda and Step Functions to automate incident response actions and mitigate damage quickly.\nQ\u0026amp;A \u0026amp; Wrap-up Summary of the 5 Pillars: A recap of the five key security pillars and their application in real-world AWS environments.\nCommon Pitfalls \u0026amp; Practices in Vietnamese Enterprises: Discussing common security challenges faced by Vietnamese companies and how to address them with AWS tools.\nSecurity Learning Roadmap: Overview of Security Specialty certification and Solutions Architect – Professional (SA Pro) certification, providing a learning path for security experts.\nWhat I Learned Security Principles \u0026amp; Pillars Core security principles: Understanding the importance of Least Privilege, Zero Trust, and Defense in Depth in the context of cloud security.\nAWS Well-Architected Security Pillars: The significance of each security pillar—Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response—in ensuring a secure cloud environment.\nCI/CD Pipeline with AWS: AWS CodeCommit: Best practices for managing source code using AWS’s version control system.\nCodeBuild \u0026amp; CodeDeploy: How to configure automated build and deployment pipelines with AWS tools.\nCodePipeline Automation: The role of AWS CodePipeline in automating the entire software delivery lifecycle.\nIAM \u0026amp; Access Control Modern IAM Architecture: Using IAM effectively to manage access securely, with tools like IAM Identity Center for SSO and MFA for strong authentication.\nMulti-account Security: How to implement SCP and permission boundaries for securing multi-account AWS environments.\nContinuous Monitoring \u0026amp; Detection CloudTrail, GuardDuty, and Security Hub: Setting up CloudTrail for activity tracking, using GuardDuty for threat detection, and aggregating security findings in Security Hub.\nLogging \u0026amp; Automation: Best practices for comprehensive logging and automated alerting with EventBridge.\nMonitoring and Observability: CloudWatch \u0026amp; X-Ray: How to implement full-stack observability in a microservices architecture using AWS CloudWatch and X-Ray for better performance tracking and issue resolution. Data Protection KMS \u0026amp; Encryption: How to implement key management and data encryption to secure sensitive data in the cloud.\nSecrets Management: Best practices for securely managing and rotating secrets with Secrets Manager and Parameter Store.\nIncident Response Automating Incident Response: Using AWS Lambda and Step Functions to automate incident response processes, improving speed and efficiency during security breaches. Application to My Work IAM Management: Implement stronger IAM policies and role-based access control (RBAC) in my organization to ensure secure and granular access management.\nContinuous Monitoring: Set up CloudTrail, GuardDuty, and Security Hub for ongoing monitoring and alerting of potential security threats.\nIncident Response Automation: Automate incident response processes using Lambda and Step Functions to reduce reaction time and improve security efficiency.\nData Encryption: Apply encryption for both data-at-rest and data-in-transit, ensuring that all sensitive data is securely handled.\nMulti-Account Security: Use SCPs and permission boundaries to manage security across multiple AWS accounts.\nExperience at the Event Learning from AWS Experts: The session provided deep insights into cloud security best practices and AWS security tools, which will help me implement effective security measures in my cloud architecture.\nHands-on Demos: I particularly enjoyed the mini demo for validating IAM policies and the incident response demo using Lambda and Step Functions, which provided practical knowledge for my own work.\nNetworking \u0026amp; Knowledge Sharing: The event allowed me to interact with security professionals, expanding my understanding of the challenges specific to cloud security in Vietnam.\nConclusion The “AWS Well-Architected Security Pillar” event was incredibly informative, providing a comprehensive understanding of cloud security best practices. The key takeaways about IAM management, continuous monitoring, data protection, and incident response automation will be directly applicable to securing our AWS environments and improving our incident response times. I now have a clearer roadmap for enhancing security practices in my organization using AWS tools.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Trần Phát Đạt\nPhone Number: 0985817735\nEmail: dattpse182773@fpt.edu.vn\nUniversity: FPT University\nMajor: Software Engineering\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Project Introduction This workshop presents the architecture and implementation of English Journey,\na vocabulary-learning web application built on AWS.\nThe application allows students to:\nsign up and sign in securely, take a level placement test (A1–C1) before learning, practise vocabulary and quizzes, track their own learning progress. On the AWS side, the project demonstrates how to combine several managed services:\nAWS Amplify as the central platform for the web app backend and hosting, Amazon Cognito for authentication, AWS Lambda for backend logic (Level test, Quiz, Vocabulary), Amazon DynamoDB for application data, Amazon SES for sending email notifications and alerts to learners (Account Verification), Amazon CloudWatch for logs, metrics, AWS WAF for basic web application protection, and IAM Roles \u0026amp; Policies to control access between all components. Workshop Objectives By the end of this workshop, a reader should be able to:\nUnderstand the overall architecture of the English Journey web app on AWS. Explain the role of Amplify and how it orchestrates Cognito, Lambda, DynamoDB and S3. Describe how the level-test feature connects frontend, Lambda and DynamoDB. Understand how notifications and system alerts are delivered via email using Amazon SES. Recognise the importance of CloudWatch and IAM for monitoring and security. Workshop Overview This project leverages AWS services to build and deploy the application:\nAWS Amplify: A full-stack hosting service that enables quick and easy deployment of applications. AWS Lambda: Handles application tasks and logic without the need to manage servers, saving costs and resources. Amazon DynamoDB: A NoSQL database used to store user data, vocabulary, and learning results. Amazon S3: Stores learning materials (videos, audio, images) to support the learning process. Amazon CloudWatch: Monitors the performance and operation of the application, providing logs and alerts in case of issues. !\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Understand and connect EC2, RDS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get to know FCJ members - Read and take note of the rules and regulations at the internship unit 08/09/2025 08/09/2025 3 - Learn about AWS and its service categories + Compute + Storage + Networking + Database - Create an AWS account + Set up MFA + Create AdminGroup and AdminUser + Verify the account + Explore and configure AWS Management Console 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about Amazon Virtual Private Cloud (VPC) and AWS Site-to-Site VPN - Practice: + Understand VPC firewall concepts and preparation steps + Debug VPC flow logs lab with Mr. Thịnh + Deploy an Amazon EC2 Instance + Configure Site-to-Site VPN 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Continue learning about EC2: - Practice: + Launch a Windows instance + Launch a Linux instance + Hands-on with Amazon EC2 + Deploy Node.js on Amazon Linux + Run a Node.js app on Amazon EC2 Windows + Learn about resource usage limits with IAM service 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about Amazon Relational Database Service (Amazon RDS) - Practice: + Create an EC2 instance + Create an RDS instance + Deploy an application + Backup and restore 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\nUnderstanding the basics of EC2:\nUnderstand how to create and manage virtual servers on the cloud. Learn how to connect via SSH/RDP to work directly with the server. Install and deploy web applications (Node.js) on EC2. Manage security, firewalls (Security Groups), and static IPs (Elastic IP). Learn how to optimize cloud costs (choose the right instance type, start/stop at the right time). Amazon RDS provides the following key benefits:\nAn easy replacement for traditional database instances. Automated backups and patching within a customer-defined maintenance window. Scaling, replication, and high availability with just one click. "
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": " On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learning and practicing EC2, VPC, AWS networking services; configuring VPN; project team meeting.\nWeek 3: Learning AWS Compute VM Services and Practicing AWS Backup, File Storage Gateway, and Autoscaling Group\nWeek 4: Learning AWS Storage Services and Practicing VM Import/Export, File Storage Gateway\nWeek 5: Learning AWS Security Services and Practicing Security Hub, EC2 Cost Optimization, and Resource Management\nWeek 6: Practicing IAM \u0026amp; KMS Labs and Learning AWS Database Services\nWeek 7: Practicing Module 6 Labs and Working on the Group Project\nWeek 8: Reviewing and Preparing for the Midterm Exam\nWeek 9: Continuing Review and Preparation for the Midterm Exam\nWeek 10: Researching and Developing the Group Project in First Cloud Journey\nWeek 11: Continued progress in researching and implementing the group project.\nWeek 12: Continued progress in researching and implementing the group project.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "This section summarizes the contents of the workshop you plan to conduct.\nStudying English Website 1. Executive Summary The Studying English Website is designed for learners aiming to improve their vocabulary, grammar, and daily communication skills. The platform leverages AWS Serverless services to provide learning time monitoring, predictive analysis of learners’ abilities to offer learning policies from basic to advanced levels, while minimizing costs.\n2. Problem Statement Current Problem\nEnglish is an essential language for work and daily life. However, learners currently lack space and an environment for practice, especially for communication.\nSolution\nTo address the lack of an English practice environment and support learners in improving vocabulary, grammar, and communication skills, we propose building the Studying English Website on the AWS serverless platform. It enables personalized learning based on user data, integrates listening-speaking exercises and instructional videos with learner recordings stored on S3, tracks and analyzes learning progress via AWS Lambda, ensures security and user management through Cognito, IAM, and rapidly deploys a cost-effective web interface using AWS Amplify, providing a flexible, safe, and effective learning environment while helping administrators improve teaching methods based on real data.\nBenefits and Return on Investment (ROI)\nThe Studying English Website helps learners enhance their English skills in a personalized and flexible way, reducing time and cost compared to traditional learning methods. It also provides learning progress analytics to administrators for optimized teaching methods. With low AWS infrastructure costs (~6.45 USD/month), the project has a fast ROI potential through increased learning efficiency and expanded user base, while creating valuable data for AI projects and long-term analytics.\n3. Solution Architecture The architecture of the Studying English Website is based on the AWS serverless platform, using S3 to store raw and processed data, Amplify Gen 2 for web interface deployment Route53 for DNS and routing management, Cognito for user authentication and management, Secrets Manager for sensitive information security, IAM for access control, Lambda for event-driven serverless logic, and WAF to protect the application from attacks, creating a flexible, personalized, secure, and scalable English learning system.\nAWS Services Used\nAWS Amplify Gen 2: Host the web interface AWS Route53: Manage DNS and routing AWS Cognito: Authenticate and manage users AWS IAM: Manage AWS access permissions AWS Lambda: Run serverless code triggered by events AWS WAF: Protect the web application from attacks Component Design\nData Ingestion: Data from users and sources is sent to AWS Lambda, which triggers processing workflows. Data Storage: Raw and processed data are stored in many separate S3 buckets, forming a data lake and ready-to-analyze data repository. Data Processing: AWS Lambda handles serverless events, MediaConvert converts video/audio, and data is indexed. Web Interface: AWS Amplify Gen 2 hosts a Next.js application providing dashboards, real-time analytics, and user data access. User Management: Amazon Cognito handles authentication and user access management, combined with AWS IAM for service access control, securing sensitive information via AWS Secrets Manager, and protecting the entire application with AWS WAF. DNS and routing are managed by Route53. 4. Technical Deployment Deployment Phases\nThe project consists of two parts — building the Studying English Website — each with four phases:\nResearch and Architecture Design: Study and design AWS Serverless architecture (1 month before internship). Cost Calculation and Feasibility Check: Use AWS Pricing Calculator to estimate infrastructure costs and adjust services to ensure feasibility and cost-efficiency (Month 1). Architecture Adjustment for Cost/Performance Optimization: Refine services (e.g., optimize Lambda, MediaConvert, Amplify) and data workflows for maximum efficiency (Month 2). Development, Testing, Deployment: Deploy AWS services using CDK/SDK, develop Next.js interface on Amplify, test the system, and put it into operation (Month 2–3). Technical Requirements\nThe system requires a stable internet connection to operate AWS services, including storing and retrieving data on S3, serverless data processing via Lambda, deploying Next.js web interface on Amplify Gen 2, DNS and routing management via Route53, user authentication and access control with Cognito, sensitive information security via Secrets Manager, service access control via IAM, WAF application protection, as well as supporting data analytics and real-time dashboards.\n5. Roadmap \u0026amp; Milestones Before Internship (Month 0): Study plan preparation Internship (Month 1–3): Month 1: Learn AWS and upgrade hardware Month 2: Learn deployment, plan and design architecture Month 3: Deploy, test, and launch Post Deployment: Research potential development and new features 6. Budget Estimation See costs on AWS Pricing Calculator\nOr download budget estimation file.\nInfrastructure Costs\nAWS Amplify Gen 2: 0.35 USD/month (256 MB, 500 ms request) AWS Route53: 0.50 USD/month (1 domain, 1 million queries) AWS Cognito: 0.00 USD/month (5 Free Tier users) AWS IAM: 0 USD/month AWS Lambda: 0.00 USD/month (1,000 requests, 512 MB RAM) AWS WAF: 5.00 USD/month (1 basic Web ACL) Total: 5.85 USD/month, ~70.2 USD/12 months\n7. Risk Assessment Risk Matrix\nServer downtime: High impact, medium probability Budget overrun: Medium impact, high probability Mitigation Strategy\nCosts: Use AWS Budget for alerts, optimize services Contingency Plan\nRevert to manual data collection if AWS services fail. 8. Expected Outcomes Technical Improvement: Real-time data and analytics replace manual processes. Scalable to 10–15 stations.\nLong-term Value: One-year data platform for AI research, reusable for future projects.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Research and work on the group project 11/03/2025 11/07/2025 3 - Research and work on the group project 11/03/2025 11/07/2025 4 - Research and work on the group project 11/03/2025 11/07/2025 5 - Research and work on the group project 11/03/2025 11/07/2025 6 - Research and work on the group project 11/03/2025 11/07/2025 Week 10 Achievements: Making good progress in researching and working on the group project. "
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Research and work on the group project 11/10/2025 11/14/2025 3 - Research and work on the group project 11/10/2025 11/14/2025 4 - Research and work on the group project 11/10/2025 11/14/2025 5 - Research and work on the group project 11/10/2025 11/14/2025 6 - Research and work on the group project 11/10/2025 11/14/2025 Week 10 Achievements: Making good progress in researching and working on the group project. "
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Attend AWS Cloud Mastery Series #2 event 11/17/2025 11/17/2025 3 - Continue researching and working on the group project 11/18/2025 11/21/2025 4 - Continue researching and working on the group project 11/18/2025 11/21/2025 5 - Continue researching and working on the group project 11/18/2025 11/21/2025 6 - Continue researching and working on the group project 11/18/2025 11/21/2025 Week 12 Achievements: *Making good progress in researching and working on the group project.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;CognitoPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cognito-idp:AdminCreateUser\u0026#34;, \u0026#34;cognito-idp:AdminUpdateUserAttributes\u0026#34;, \u0026#34;cognito-idp:ListUsers\u0026#34;, \u0026#34;cognito-idp:SignUp\u0026#34;, \u0026#34;cognito-idp:InitiateAuth\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;LambdaPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:ListFunctions\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:UpdateFunctionCode\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;DynamoDBPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:region:account-id:table/your-table-name\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SESPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ses:SendEmail\u0026#34;, \u0026#34;ses:SendRawEmail\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } { \u0026#34;Sid\u0026#34;: \u0026#34;CloudWatchPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudwatch:PutMetricData\u0026#34;, \u0026#34;cloudwatch:GetMetricData\u0026#34;, \u0026#34;cloudwatch:DescribeAlarms\u0026#34;, \u0026#34;cloudwatch:SetAlarmState\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;WAFPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;wafv2:CreateWebACL\u0026#34;, \u0026#34;wafv2:UpdateWebACL\u0026#34;, \u0026#34;wafv2:GetWebACL\u0026#34;, \u0026#34;wafv2:ListWebACLs\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;IAMPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:ListPolicies\u0026#34;, \u0026#34;iam:GetRole\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Technical background This workshop assumes that the reader has basic knowledge of:\nWeb development\nHTML, CSS and JavaScript/TypeScript React or another single-page application framework Cloud and AWS fundamentals\nwhat an AWS region and account are, the idea of managed services (Cognito, Lambda, DynamoDB,…), basic concepts of IAM (identity, role, policy, least privilege). The report is written so that it can be understood even if the reader does not actually deploy the system, but this background helps to follow the architecture.\nTools and services To reproduce the workshop in a real environment, the following tools and services would be required:\nAn AWS account with permission to create:\nAmplify apps, Cognito User Pools, Lambda functions, DynamoDB tables, SES identities and configuration sets, IAM roles and policies. Node.js and npm installed locally\n(for running and building the React frontend).\nThe AWS CLI configured with an IAM user or role that has sufficient permissions.\nOptionally, the Amplify CLI / Gen 2 tooling\nto define infrastructure in code and connect the project to Amplify.\nSource code and project structure The English Journey project is organised as:\na React frontend (pages such as Level Test, Dictionary, Vocabulary, My Learning), a backend defined via Amplify (Cognito, Lambda, DynamoDB), additional infrastructure for SES (email), CloudWatch and WAF. In this Hugo workshop site, we only present the architecture diagrams, explanations and example code. The actual AWS resources do not need to be created to understand the design decisions.\nThe following sections (5.3 and onwards) build on these prerequisites and explain each group of AWS services in more detail.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/5-workshop/5.3-create-amplify/",
	"title": "Create Amplify backend",
	"tags": [],
	"description": "",
	"content": "Goal In this step we describe how the Amplify backend for the English Journey web application was created.\nInstead of provisioning Amazon Cognito, AWS Lambda, Amazon S3, AWS WAF and Amazon DynamoDB separately in the console, we use AWS Amplify (Gen 2) as a central orchestration layer. Amplify reads the configuration from our project and generates the necessary AWS resources for authentication, APIs, data storage and hosting.\n5.3.1 Why Amplify? English Journey is a single-page application built with React. We chose Amplify because:\nit provides one entry point to configure the backend for a web or mobile app, it can automatically create Cognito, Lambda, DynamoDB, S3 and CloudFront resources from a simple configuration, it integrates easily with the Amplify JavaScript libraries used by the frontend (sign-in, API calls, file storage). Amplify becomes the “backend platform” that hides much of the low-level boilerplate of setting up these services one by one.\n5.3.2 Amplify app creation and hosting We created a new Amplify App from the AWS Management Console and connected it to our Git repository that contains the React source code of English Journey. During the wizard we configured: the default build settings (install dependencies and run npm run build), the environment variables required by the app. Amplify automatically created: an S3 bucket to store the built static files, a CloudFront distribution in front of that bucket to serve the web site with low latency. The output of this step is a public URL where the React frontend of English Journey is hosted. All later backend resources (Cognito, Lambda, DynamoDB…) are associated with this Amplify app. 5.3.3 Authentication with Cognito (Amplify Auth) To implement sign-up, sign-in and password reset for students, we enabled the Auth category in Amplify.\nConceptually, the steps were:\nDefine the authentication requirement in Amplify: sign-in with email and password, self-registration enabled so that new students can create accounts, basic password policy and email verification. Amplify generated an Amazon Cognito User Pool with the corresponding settings. The React frontend uses the Amplify Auth library to: create new users (sign up), authenticate existing users (sign in), read user attributes (name, email) and display greetings such as “Chào mừng trở lại, Duy Khang!”. All tokens issued by Cognito (ID token and access token) are later used to protect our API and Lambda functions.\n5.3.4 Backend logic with Lambda (Amplify Functions) The main business logic of English Journey runs in AWS Lambda.\nUsing Amplify’s Function category we created several Lambda functions, for example:\nMyLearning / DailyCheckIn – updates study streaks and progress for the user. LevelTest – receives the answers from the placement test, calculates the CEFR level (A1–C2) and stores the result. Dictionary / Vocabulary – provides APIs for searching words, saving “bookmarked” vocabulary and tracking which words a user has mastered. From the Amplify project these functions are defined as backend handlers.\nWhen deploying the Amplify app, each function is created as a separate Lambda with the correct IAM permissions and environment variables (for example, DynamoDB table names).\n5.3.5 Data layer with DynamoDB To store application data we defined several data models in the backend, which Amplify maps to Amazon DynamoDB tables:\nUsers / Profiles – basic user information and learning preferences. PlacementTestResults – scores and detected level for each attempt. Vocabulary / Dictionary – list of words, meanings, examples and CEFR levels. UserProgress – saved vocabulary, words marked as “mastered”, quiz history, daily streaks. Each Lambda function receives the table name via environment variables generated by Amplify, and accesses DynamoDB through the AWS SDK.\nUsing Amplify in this way keeps all table definitions in code and makes the deployment reproducible.\n5.3.7 Protection with AWS WAF The public endpoint of the web application is the CloudFront distribution created by Amplify.\nTo protect this endpoint we associated an AWS WAF Web ACL with the distribution and enabled:\nthe AWS managed rule groups that block common web attacks (SQL injection, XSS, bot traffic), a basic rate-limit rule to prevent simple denial-of-service attempts. In the architecture diagram this is represented by the AWS WAF component in front of the Amplify application.\nSummary In summary, Amplify is the central service that creates and connects:\nCognito for authentication, Lambda for backend logic, DynamoDB for application data, and integrates with AWS WAF for additional protection. The rest of the workshop (SNS, CloudWatch, IAM policies, …) builds on top of this Amplify-managed backend.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": " This section will list and introduce the blogs you have translated. For example:\nBlog 1 – Deploying a High Performance Computing solution for accurate weather and renewable energy production predictions This blog explains how Iberdrola deploys a high-performance computing (HPC) cluster on AWS to improve weather forecasting accuracy and renewable energy production predictions. The article walks through business motivations, MeteoFlow’s evolution, data requirements, cloud migration, HPC architecture, and AWS services that enable large-scale meteorological simulations and ML-based forecasting.\nBlog 2 - Showcasing Your AWS Achievements with the New Skills Profile This blog introduces Skills Profile, a new feature in AWS Skill Builder that allows learners to compile, manage, and share all their AWS certifications and achievements in a single profile. The tool helps showcase skills in a professional and verifiable way, making it ideal for sharing on platforms like LinkedIn or in job applications. As a result, learners can enhance their credibility, expand their network, and create more career opportunities in the cloud computing field.\nBlog 3 - How DTN accelerates operational weather prediction using NVIDIA Earth-2 on AWS This blog describes how DTN uses NVIDIA Earth-2 on AWS to accelerate operational weather forecasting with AI models, especially for predicting and tracking cyclones. By deploying AI models like FourCastNet on AWS services such as AWS Batch, GPU-powered EC2 instances, Step Functions, and S3, DTN can generate forecasts orders of magnitude faster than traditional physics-based models while improving accuracy and scalability. This solution helps businesses reduce risk, optimize operations, and make better decisions during extreme weather events.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/5-workshop/5.4-aws-cognito/",
	"title": "Configure AWS Cognito",
	"tags": [],
	"description": "",
	"content": "Goal In this step, we describe how Amazon Cognito is used in the English Journey web application to manage user authentication.\nStudents can sign in using:\nEmail \u0026amp; password Google (Gmail) account Cognito acts as the central identity provider that issues tokens for the frontend and backend of English Journey.\n5.4.1 Role of Amazon Cognito in the architecture In the architecture of English Journey:\nCognito User Pool stores user identities (email, name, etc.). It handles sign-up, sign-in, password reset and email verification. It integrates with Amplify so the React frontend can easily call signIn, signUp and other auth functions. It also connects to social identity providers: Google → for users who want to log in with their Gmail All successful logins (email, Google) result in Cognito tokens that are used to protect our APIs and Lambda functions.\n5.4.2 Creating the Cognito User Pool The main configuration steps:\nCreate a User Pool\nSign in to the AWS Management Console → Amazon Cognito → User pools → Create user pool. Choose Email as the primary sign-in identifier. Allow self-registration so students can create their own accounts. Configure password policy \u0026amp; verification\nSet a basic password policy (minimum length, characters, etc.). Enable email verification so students must confirm their email before using the app. Customize email templates (optional) for sign-up and password reset. Create an app client\nCreate a public app client for the web frontend. Enable Cognito User Pool and social IdPs as allowed identity providers. Configure allowed callback URLs and sign-out URLs (for example, the Amplify frontend domain of English Journey). This user pool is later referenced by Amplify and the React frontend.\n5.4.3 Enabling Google (Gmail) To support social logins, we added Google as identity providers in Cognito.\nGoogle (Gmail) login In Google Cloud Console, create an OAuth 2.0 Client ID for a web application. Set the authorized redirect URI to the Cognito callback URL generated for the user pool. Copy the Client ID and Client Secret from Google. In Cognito → User pool → Identity providers → Google: Paste the Client ID and Client Secret. Map Google attributes (email, name) to Cognito standard attributes. Now users can click \u0026ldquo;Sign in with Google\u0026rdquo; and authenticate using their Gmail account.\n5.4.4 Integrating Cognito with the Amplify frontend The React frontend of English Journey uses AWS Amplify Auth to communicate with Cognito.\nConceptually:\nFor email/password: Auth.signUp() is used to create a new user. Auth.signIn() is used for normal login. For Gmail social login: We call Auth.federatedSignIn({ provider: 'Google' }), Amplify redirects the user to Google, then back to Cognito, then back to the web app with valid tokens. On the UI, the login page shows three main options:\nSign in with email \u0026amp; password\rSign in with Google\rAll three methods still end up in the same Cognito User Pool.\n5.4.5 Security and user management With Cognito, we can:\nRequire email verification before granting full access to the application.\nRestrict which domains are allowed to use the sign-in flow (via callback URLs).\nManage users centrally:\nLock accounts Reset passwords Delete accounts Extend the solution later with:\nMFA (Multi-Factor Authentication) Additional social providers (GitHub, Facebook, …) Within the scope of this workshop, we focus on:\nSign-in with email \u0026amp; password Application login via Gmail Email verification (OTP) Summary In this step, we configure Amazon Cognito as the identity management service for English Journey:\nCreate a User Pool to manage accounts and authenticate users. Allow users to sign in with email and Google (Gmail). Use tokens issued by Cognito in the React + Amplify front end to securely access APIs and back-end services. "
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: DevOps on AWS\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/5-workshop/5.5-create-ses/",
	"title": "Configure Amazon SES for email",
	"tags": [],
	"description": "",
	"content": "Goal SES will be used to send transactional emails to learners\nIt can later be extended to:\nSend welcome emails when an account is created. Send study reminder emails. Note: SES is a regional service. Make sure you are working in the same AWS Region that you used for Amplify, Lambda and DynamoDB.\n5.5.1 – Open the Amazon SES console From the AWS Management Console, type “SES” in the search box. Select Amazon Simple Email Service. Check the Region in the top-right corner (for example: ap-southeast-1). If SES is in a different Region, switch to the Region used for this workshop environment. 5.5.2 – Verify an email identity For this workshop we will verify a single email address and send emails from that address (for example: your personal Gmail).\nIn the SES left navigation menu, choose Verified identities. Click Create identity. Select Email address. In Email address, enter the address you will use as the sender, for example:\nyour-name+english-journey@gmail.com. Leave the other options as default and choose Create identity. SES sends a verification email to that address. Open your inbox, find the email from Amazon Web Services, and click the verification link. Return to the SES console and refresh. The identity status should become Verified. From now on, SES will only allow sending emails from and to verified identities (SES Sandbox mode). This is enough for a classroom / workshop environment.\n5.5.3 – (Optional) Move out of Sandbox mode If later you want to send emails to real learners (unverified addresses), you must move your SES account out of sandbox mode:\nIn the SES console, go to Account dashboard. Under Your account details, check the Account status. If it is still Sandbox, click Request production access and follow the instructions. Within the scope of this workshop, you can stay in sandbox mode as long as you only send emails between verified addresses.\n5.5.4 – Create a configuration set (optional but recommended) A configuration set groups all emails from the English Journey application and enables future observability (CloudWatch metrics, event publishing, etc.).\nIn the SES navigation menu, choose Configuration sets. Click Create configuration set. Enter a name, for example: english-journey-config. Leave all other settings as default and click Create configuration set. We will use this configuration set later when sending emails from Lambda functions.\n5.5.5 – Allow Lambda to send email with SES Lambda functions such as Daily Check-in or Test Level result will send emails through SES.\nLambda needs permission to call the SES APIs.\nYou will attach this permission in Section 5.7 – Create IAM Roles \u0026amp; Policies, but we prepare the policy here for reference:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ses:SendEmail\u0026#34;, \u0026#34;ses:SendRawEmail\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "English Journey Overview English Journey is an innovative web application designed to help users learn English vocabulary in a structured and interactive way. This platform leverages various AWS services to provide a smooth learning experience, allowing users to track their progress, engage with dynamic content, and receive personalized feedback.\nKey Features: User Authentication: Using Amazon Cognito, users can register, log in, and securely access their learning materials.\nInteractive Learning Modules: The app offers various interactive lessons to help users expand their vocabulary.\nProgress Tracking: Users can track their progress and completion of vocabulary lessons, with detailed reports generated through AWS Lambda and DynamoDB.\nNotifications: Email notifications via Amazon SES will inform users about new lessons, progress milestones, account updates, and other important changes.\nContent Storage: All learning materials are securely stored in Amazon S3 with proper access control.\nWeb Security: To protect the platform, AWS WAF ensures the application is protected from common web threats.\nMonitoring and Alerts: AWS CloudWatch is used to monitor platform performance, with alerts configured for potential issues.\nTechnologies Used: Frontend: Built using modern web technologies, ensuring a smooth and responsive user experience.\nBackend: Powered by AWS services like Lambda and DynamoDB, ensuring scalability and performance.\nStorage: All data and media content are securely stored in Amazon S3.\nContent Workshop overview Prerequiste Create Amplify AWS Cognito Configure Amazon SES for email CloudWatch IAM Roles - Policies Configure Amazon Route 53 Clean up "
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/5-workshop/5.6-create-cloudwatch/",
	"title": "Configure Amazon CloudWatch",
	"tags": [],
	"description": "",
	"content": "Goal To understand how the application behaves in production and to be able to react quickly to errors, we use Amazon CloudWatch for:\ncollecting logs from Lambda functions, monitoring metrics (invocations, errors, throttles, latency), 5.6.1 CloudWatch Logs for Lambda and API By default, every Lambda function created by Amplify writes logs to CloudWatch Logs.\nIn this project, logs are used to:\nTrack requests for: Level test submissions, Quiz submissions, Dictionary / vocabulary lookups, Debug input errors, permission (IAM) errors or timeouts, Record important application events. In addition to the default logs, some functions write structured JSON logs, which makes it easier to search by userId, requestId or feature.\n5.6.2 Metrics for key services CloudWatch automatically provides metrics for:\nLambda – invocations, errors, duration, concurrent executions, DynamoDB – read/write capacity, throttled requests, S3 / CloudFront – data transfer and request counts, WAF – number of allowed/blocked requests. For the workshop we focus on a few key metrics:\nLambda Error count and Error rate for the main backend functions, Lambda Duration to detect performance issues in level test \u0026amp; quiz processing, DynamoDB ThrottledRequests to see if the provisioned capacity is sufficient. 5.6.3 CloudWatch Dashboard (optional) To quickly observe the system health, the team creates a small CloudWatch Dashboard showing:\nA chart of Lambda error rate over time, Execution duration of the LevelTest and Dictionary functions, (Optional) Number of requests blocked by AWS WAF. The dashboard is not mandatory for the workshop, but it helps illustrate the system behavior when many students are using the application.\nSummary CloudWatch completes the monitoring story for English Journey by providing:\nLogs for analysis and debugging, Metrics \u0026amp; dashboards to observe trends, Combined with Amplify, SES and WAF, this gives a reasonably robust operational setup for this workshop project.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": " During my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Need to further enhance my knowledge base and effectively leverage tools to support my work Strive to improve my analytical thinking and problem-solving approach Aim to be more proactive in contributing ideas during team collaboration "
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/5-workshop/5.7-create-iam-roles-policies/",
	"title": "Create IAM Roles-Policies",
	"tags": [],
	"description": "",
	"content": "Goal This section explains how IAM roles and policies are designed for the English Journey application.\nMost roles are generated by AWS Amplify, but we still need to understand:\nwhich roles exist, what each role is allowed to do (S3, DynamoDB, SES, MediaConvert, …), and how we apply the least-privilege principle. 5.7.1 Overview of IAM in the architecture In the architecture from sections 5.3–5.6, IAM is the glue that connects services:\nAmplify uses IAM roles to deploy CloudFormation stacks and host the frontend. Lambda functions use execution roles to access DynamoDB, S3 and SES (for sending emails). CloudWatch and SES rely on IAM so alarms and email notifications can be sent correctly. The design goal is that each component only receives the minimum permissions it needs.\n5.7.2 Lambda execution roles When we define backend functions in Amplify (Level Test, My Learning, Dictionary, Vocabulary, …), Amplify automatically creates a Lambda execution role for each function.\nEach role has:\nTrust policy – allows the Lambda service to assume the role:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } "
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/7-feedback/",
	"title": "Feedback &amp; Suggestions",
	"tags": [],
	"description": "",
	"content": " Here you can freely share your personal opinions and experiences during the First Cloud Journey program to help the FCJ team improve based on the following criteria:\nOverall Evaluation 1. Working Environment\nThe working environment is welcoming and highly collaborative. FCJ team members are always ready to assist whenever I run into challenges — even beyond regular working hours. The workspace is well-organized and comfortable, which helps me stay focused and productive. Still, I believe that adding more team bonding activities or informal gatherings would help everyone connect and understand each other better.\n2. Support from Mentor / Admin Team\nMy mentor offers clear, thorough guidance and takes the time to explain concepts whenever I struggle to understand something. They also encourage me to ask questions and explore different approaches. The admin team is very supportive with documentation, administrative processes, and day-to-day logistics, making the internship experience seamless. I especially appreciate that my mentor allows me to experiment and solve problems independently rather than simply providing the answers.\n3. Alignment Between Work and Academic Background\nThe tasks assigned to me align well with what I have studied in school while also exposing me to new concepts and tools that I had not encountered before. This combination helped reinforce my academic foundation and develop practical, industry-relevant skills.\n4. Learning Opportunities \u0026amp; Skill Development\nDuring the internship, I gained valuable experience with project management tools, strengthened my teamwork abilities, and improved my professional communication in a corporate setting. My mentor also shared practical industry insights that helped me better understand and shape my career direction.\n5. Company Culture \u0026amp; Team Spirit\nThe company fosters a very positive culture where everyone is respectful, supportive, and dedicated. Despite the serious approach to work, the atmosphere remains friendly and energetic. When urgent tasks arise, the whole team collaborates effectively, offering help regardless of roles or seniority. This made me feel truly integrated into the team, even as an intern.\n6. Intern Policies / Benefits\nThe company provides internship allowances and accommodates flexible working hours when necessary. Moreover, being able to participate in internal training sessions is a significant advantage that greatly contributed to my professional growth.\nAdditional Questions What were you most satisfied with during the internship?\nI was most satisfied with the strong support from my mentor and the friendly, open working environment that helped me learn and grow quickly.\nWhat do you think the company should improve for future interns?\nI think it would be beneficial to introduce more mini-projects or specialized workshops so interns can gain more hands-on experience with real-world scenarios.\nWould you recommend this internship program to your friends? Why or why not?\nYes. Because it offers a great learning environment, supportive mentors, and opportunities to work with modern cloud technologies.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\nI suggest adding more cross-team sharing sessions and offering multi-level training paths so interns can follow a clearer learning roadmap.\nWould you like to continue with this program in the future?\nYes, if given the opportunity, I would like to continue to deepen my knowledge and gain more practical experience.\nOther comments (free sharing):\nThank you to the FCJ team for your continuous support throughout my internship. I hope the program will keep growing and bring value to many more students in the future.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/5-workshop/5.8-route-53/",
	"title": "Configure Amazon Route 53",
	"tags": [],
	"description": "",
	"content": "5.8 Configure Amazon Route 53 (Custom Domain) In this step we connect the Amplify-hosted English Journey frontend to a custom domain managed by Amazon Route 53.\n🔗 Domain used in this workshop\nFor the demo environment we use the domain\nenglishjourney.xyz – the final site is available at:\nhttps://www.englishjourney.xyz/\n5.8.1 Create / verify the hosted zone Open the Route 53 console → Hosted zones → Create hosted zone. Enter your domain name, e.g. englishjourney.xyz, and keep type = Public hosted zone. Route 53 creates a set of NS and SOA records for the zone. If the domain is registered elsewhere, copy the Route 53 NS records to your registrar so that DNS is delegated to Route 53. 5.8.2 Connect the domain in AWS Amplify Go to the AWS Amplify console → select your English Journey app.\nIn the left menu choose Domain management → Add domain.\nSelect the hosted zone englishjourney.xyz.\nMap the root and sub-paths, for example:\nenglishjourney.xyz → main branch (production) www.englishjourney.xyz → redirect to root Amplify automatically creates the required A / AAAA and CNAME records in Route 53.\n5.8.3 Test the site Wait for DNS and SSL provisioning to complete (a few minutes).\nOpen a browser and navigate to:\nhttps://www.englishjourney.xyz/ Verify that the English Journey homepage is served correctly over HTTPS.\nNote this URL in your report / slides as the public entry point of the workshop application.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/5-workshop/5.9-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Goal This section explains how to remove the AWS resources that were created or used during the English Journey workshop, so that you do not incur unexpected costs.\nYou should only delete these resources when you have finished experimenting with the architecture.\n5.9.1 – Delete the Amplify app and front-end hosting Open the Amplify console in the same Region used for the workshop. Select the Amplify app that is hosting the English Journey front end. Choose Actions → Delete app (or the Delete button in the app details page). Confirm the deletion as instructed. When you delete the Amplify app:\nAmplify automatically removes the front-end hosting, And usually deletes the backend stacks it created (Cognito, Lambda, DynamoDB),\nunless you explicitly choose to keep them during the deletion process. Carefully read the confirmation dialog to avoid deleting important resources by mistake.\n5.9.2 – Delete any remaining back-end resources Depending on how you created the back end, there may still be some resources left after deleting the Amplify app.\nIn the AWS console, in the workshop Region, check the following services:\nCognito\nDelete any User Pools or Identity Pools that were created specifically for the workshop. Lambda\nDelete Lambda functions that only serve English Journey (for example: level test handlers, daily reminders, vocabulary processing). DynamoDB\nDelete DynamoDB tables that were used only for workshop data (learning progress, questions, vocabulary, …) if you no longer need them. 5.9.3 – Clean up SES, CloudWatch and WAF In addition to the core backend, this workshop uses Amazon SES, CloudWatch and optionally AWS WAF.\nAmazon SES Open the Amazon SES console. In Verified identities: Delete email identities that were created only for the workshop (for example: test sender or test recipient addresses). In Configuration sets: Delete the configuration set used by the English Journey application (for example: english-journey-config), if you will not reuse it. If your account was moved out of SES sandbox only for the workshop, you may want to review your SES sending quotas and usage, but there is nothing extra to delete for that.\nCloudWatch Open the CloudWatch console. In Log groups, delete: Log groups for Lambda functions that belong to English Journey. AWS WAF If you deployed a dedicated WAF Web ACL for the English Journey frontend:\nOpen the AWS WAF console. Identify the Web ACL associated with the workshop CloudFront distribution or Amplify app. If the Web ACL is used exclusively for this workshop, delete it. 5.9.4 – Clean up IAM roles and policies Finally, review IAM to ensure there are no unused roles or policies left behind:\nIn the IAM console, go to Roles:\nLook for roles created only for this workshop (for example: custom Lambda execution roles, or roles with names that clearly reference English Journey or the workshop). Before deleting a role, confirm that no Lambda function, service or user still depends on it. In Policies:\nRemove customer-managed policies that were created solely for the workshop, especially: policies that grant ses:SendEmail / ses:SendRawEmail to Lambda, policies used only by temporary roles. Do not delete shared or production IAM roles / policies that might be reused by other applications.\nAfter these steps, the AWS environment should no longer contain resources that were created specifically for the English Journey workshop.\n"
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://dattpse182773.github.io/AWS_FA25_Workshop/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]